import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import random
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# üîí –§–∏–∫—Å–∞—Ü–∏—è —Å–∏–¥–æ–≤
np.random.seed(42)
random.seed(42)
tf.random.set_seed(42)

# üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
dataset = pd.read_csv("LST_final_TRUE.csv")
features = ["H", "TWI", "Aspect", "Hillshade", "Roughness", "Slope",
            "Temperature_merra_1000hpa", "Time", "DayOfYear", "X", "Y"]
target = "T_rp5"

X = dataset[features].values
y = dataset[target].values

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# ‚úÖ –õ—É—á—à–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
epochs = 300
batch_size = 32
neurons = (128, 64, 32)
activation = 'elu'
regularization = ''
optimizer = 'adam'
learning_rate = 0.001

# üîÅ –§—É–Ω–∫—Ü–∏—è –º–æ–¥–µ–ª–∏ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
def run_model(epochs, batch_size, neurons, activation, regularization, optimizer, learning_rate):
    model = keras.Sequential()
    model.add(layers.Dense(neurons[0], activation=activation, input_shape=(X_train.shape[1],)))
    if regularization == 'dropout':
        model.add(layers.Dropout(0.2))
    model.add(layers.Dense(neurons[1], activation=activation))
    if regularization == 'dropout':
        model.add(layers.Dropout(0.2))
    model.add(layers.Dense(neurons[2], activation=activation))
    if regularization == 'dropout':
        model.add(layers.Dropout(0.2))
    model.add(layers.Dense(1))

    if optimizer == 'adam':
        opt = keras.optimizers.Adam(learning_rate=learning_rate)
    else:
        raise ValueError("–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä")

    model.compile(optimizer=opt, loss="mse", metrics=["mae"])
    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,
                        validation_data=(X_test, y_test), verbose=1)

    predictions = model.predict(X_test).flatten()
    mse = mean_squared_error(y_test, predictions)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, predictions)
    mape = np.mean(np.abs((y_test - predictions) / y_test)) * 100
    r2 = r2_score(y_test, predictions)

    return mse, rmse, mae, mape, r2, history

# üöÄ –ó–∞–ø—É—Å–∫
mse, rmse, mae, mape, r2, history = run_model(epochs, batch_size, neurons, activation, regularization, optimizer, learning_rate)

# üìã –í—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫
print(f"MSE: {mse:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"MAE: {mae:.4f}")
print(f"MAPE: {mape:.2f}%")
print(f"R¬≤: {r2:.4f}")

# üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
results_df = pd.DataFrame([{
    "Epochs": epochs,
    "Batch Size": batch_size,
    "Neurons": str(neurons),
    "Activation": activation,
    "Regularization": regularization,
    "Optimizer": optimizer,
    "Learning Rate": learning_rate,
    "MSE": mse,
    "RMSE": rmse,
    "MAE": mae,
    "MAPE": mape,
    "R¬≤": r2
}])
results_df.to_csv("best_model_metrics.csv", index=False)

# üìà –ì—Ä–∞—Ñ–∏–∫–∏: MSE vs Epochs
plt.figure(figsize=(10, 6))
sns.lineplot(x=np.arange(1, epochs+1), y=history.history['loss'], label='MSE (Train)')
sns.lineplot(x=np.arange(1, epochs+1), y=history.history['val_loss'], label='MSE (Val)')
plt.title("MSE –ø–æ —ç–ø–æ—Ö–∞–º")
plt.xlabel("–≠–ø–æ—Ö–∏")
plt.ylabel("MSE")
plt.legend()
plt.show()

# üìà –ì—Ä–∞—Ñ–∏–∫–∏: MAE vs Epochs
plt.figure(figsize=(10, 6))
sns.lineplot(x=np.arange(1, epochs+1), y=history.history['mae'], label='MAE (Train)')
sns.lineplot(x=np.arange(1, epochs+1), y=history.history['val_mae'], label='MAE (Val)')
plt.title("MAE –ø–æ —ç–ø–æ—Ö–∞–º")
plt.xlabel("–≠–ø–æ—Ö–∏")
plt.ylabel("MAE")
plt.legend()
plt.show()


# Epochs,Batch Size,Neurons,Activation,Regularization,Optimizer,Learning Rate,MSE,RMSE,MAE,MAPE,R¬≤
# 300,32,"(128, 64, 32)",elu,,adam,0.001,7.188101821599612,2.6810635616485508,2.011768555766132,11.193297449022605,0.8093724614557449

